{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a5e9086",
      "metadata": {
        "id": "9a5e9086"
      },
      "source": [
        "# Análise e Previsão de Churn de Clientes - TelecomX\n",
        "\n",
        "Este notebook consolida todas as etapas do projeto de previsão de churn para a TelecomX, desde a análise exploratória de dados (EDA) até a otimização e avaliação de modelos de Machine Learning. O objetivo é identificar clientes com alta probabilidade de cancelar seus serviços, permitindo à empresa implementar estratégias de retenção proativas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775fad55",
      "metadata": {
        "id": "775fad55"
      },
      "source": [
        "## 1. Configuração e Carregamento de Dados\n",
        "\n",
        "Nesta seção, importamos as bibliotecas necessárias e carregamos o dataset de clientes da TelecomX. O dataset está em formato JSON e contém informações aninhadas que serão desaninhadas para facilitar a análise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8ec6c5",
      "metadata": {
        "id": "3d8ec6c5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Configurações de visualização\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Caminhos para dados e modelos\n",
        "DATA_RAW_PATH = Path('../data/raw/Telco-Customer-Churn.json')\n",
        "DATA_PROCESSED_PATH = Path('../data/processed')\n",
        "MODELS_PATH = Path('../models')\n",
        "REPORTS_PATH = Path('../reports')\n",
        "FIG_DIR = REPORTS_PATH / \"figures\"\n",
        "\n",
        "# Criação de diretórios se não existirem\n",
        "DATA_PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
        "REPORTS_PATH.mkdir(parents=True, exist_ok=True)\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Função para carregar e desaninhar dados JSON\n",
        "def load_and_flatten_json(path: Path) -> pd.DataFrame:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.json_normalize(data)\n",
        "    return df\n",
        "\n",
        "df = load_and_flatten_json(DATA_RAW_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375c4a7d",
      "metadata": {
        "id": "375c4a7d"
      },
      "source": [
        "## 2. Análise Exploratória de Dados (EDA)\n",
        "\n",
        "Esta seção foca na compreensão inicial dos dados, suas distribuições, valores ausentes e a relação entre as variáveis e a variável alvo (churn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0e957d",
      "metadata": {
        "id": "8d0e957d"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0607fcd5",
      "metadata": {
        "id": "0607fcd5"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32470e97",
      "metadata": {
        "id": "32470e97"
      },
      "outputs": [],
      "source": [
        "# Preparar a variável alvo para EDA\n",
        "df_eda = df.copy()\n",
        "df_eda[\"Churn\"] = df_eda[\"Churn\"].replace({\"\": np.nan})\n",
        "df_eda[\"churn_flag\"] = df_eda[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# Distribuição de Churn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"Churn\", data=df_eda, order=[\"No\", \"Yes\"])\n",
        "plt.title(\"Distribuição de Churn (alvo)\")\n",
        "plt.xlabel(\"Churn\")\n",
        "plt.ylabel(\"Contagem\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"target_churn_distribution.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Conversões numéricas de cobranças (podem vir como string)\n",
        "for c in [\"account.Charges.Monthly\", \"account.Charges.Total\"]:\n",
        "    if c in df_eda.columns:\n",
        "        df_eda[c] = pd.to_numeric(df_eda[c], errors=\"coerce\")\n",
        "\n",
        "# Tenure\n",
        "if \"customer.tenure\" in df_eda.columns:\n",
        "    df_eda[\"customer.tenure\"] = pd.to_numeric(df_eda[\"customer.tenure\"], errors=\"coerce\")\n",
        "\n",
        "# Plotar distribuição de cobrança mensal por churn\n",
        "plt.figure(figsize=(7,4))\n",
        "sns.kdeplot(data=df_eda.dropna(subset=[\"account.Charges.Monthly\", \"Churn\"]), x=\"account.Charges.Monthly\", hue=\"Churn\", common_norm=False, fill=True)\n",
        "plt.title(\"Distribuição de cobrança mensal por churn\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"kde_monthly_by_churn.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plotar distribuição de tenure por churn\n",
        "plt.figure(figsize=(7,4))\n",
        "sns.kdeplot(data=df_eda.dropna(subset=[\"customer.tenure\", \"Churn\"]), x=\"customer.tenure\", hue=\"Churn\", common_norm=False, fill=True)\n",
        "plt.title(\"Distribuição de tenure por churn\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"kde_tenure_by_churn.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Função auxiliar para plotar taxa de churn por categoria\n",
        "def plot_categorical_churn_rate(df_plot: pd.DataFrame, col: str, title: str, fname: str):\n",
        "    if col not in df_plot.columns:\n",
        "        return\n",
        "    tmp = (\n",
        "        df_plot.dropna(subset=[col, \"churn_flag\"])\n",
        "          .groupby(col)[\"churn_flag\"].mean()\n",
        "          .sort_values(ascending=False)\n",
        "    )\n",
        "    plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=tmp.index, y=tmp.values, color=\"#3366cc\")\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"Taxa de churn (média)\")\n",
        "    plt.xlabel(col)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / fname, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# Plotar churn médio por tipo de contrato\n",
        "plot_categorical_churn_rate(df_eda, \"account.Contract\", \"Churn médio por tipo de contrato\", \"rate_by_contract.png\")\n",
        "\n",
        "# Plotar churn médio por tipo de serviço de internet\n",
        "plot_categorical_churn_rate(df_eda, \"internet.InternetService\", \"Churn médio por tipo de internet\", \"rate_by_internet_service.png\")\n",
        "\n",
        "# Sumário de valores ausentes\n",
        "missing_by_col = df_eda.isna().mean().sort_values(ascending=False).head(15)\n",
        "print(\"\n",
        "Top 15 colunas com mais valores ausentes:\")\n",
        "print(missing_by_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b729976",
      "metadata": {
        "id": "7b729976"
      },
      "source": [
        "## 3. Pré-processamento e Engenharia de Features\n",
        "\n",
        "Nesta etapa, os dados são limpos, transformados e novas features são criadas para melhorar o desempenho dos modelos de Machine Learning. Isso inclui o tratamento de valores ausentes, conversão de tipos de dados e a criação de variáveis que capturam informações mais relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff40518",
      "metadata": {
        "id": "3ff40518"
      },
      "outputs": [],
      "source": [
        "def clean_and_engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert 'TotalCharges' to numeric, coercing errors to NaN\n",
        "    df[\"account.Charges.Total\"] = pd.to_numeric(df[\"account.Charges.Total\"], errors=\"coerce\")\n",
        "\n",
        "    # Fill missing 'TotalCharges' with 0 (assuming new customers or very low usage)\n",
        "    df[\"account.Charges.Total\"] = df[\"account.Charges.Total\"].fillna(0)\n",
        "\n",
        "    # Convert 'Churn' to binary (1 for Yes, 0 for No), handle empty strings as NaN\n",
        "    df[\"Churn\"] = df[\"Churn\"].replace({\"\": np.nan})\n",
        "    df = df.dropna(subset=[\"Churn\"]) # Drop rows where Churn is NaN\n",
        "    df[\"churn_flag\"] = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "    # Drop original 'Churn' column if 'churn_flag' is created\n",
        "    df = df.drop(columns=[\"Churn\"], errors=\"ignore\")\n",
        "\n",
        "    # Feature Engineering\n",
        "    # AvgMonthlyCost = TotalCharges / Tenure (handle division by zero for tenure=0)\n",
        "    df[\"AvgMonthlyCost\"] = df.apply(lambda row: row[\"account.Charges.Total\"] / row[\"customer.tenure\"] if row[\"customer.tenure\"] != 0 else 0, axis=1)\n",
        "\n",
        "    # Total_Servicos: count of additional services\n",
        "    service_cols = [\n",
        "        \"phone.MultipleLines\", \"internet.OnlineSecurity\", \"internet.OnlineBackup\",\n",
        "        \"internet.DeviceProtection\", \"internet.TechSupport\", \"internet.StreamingTV\",\n",
        "        \"internet.StreamingMovies\"\n",
        "    ]\n",
        "    # Replace 'No phone service' and 'No internet service' with 'No' for consistency in counting\n",
        "    for col in service_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].replace({\"No phone service\": \"No\", \"No internet service\": \"No\"})\n",
        "\n",
        "    df[\"Total_Servicos\"] = df[service_cols].apply(lambda row: sum(row == \"Yes\"), axis=1)\n",
        "\n",
        "    # Idoso_Com_Dependentes: 1 if SeniorCitizen is 1 and Dependents is 'Yes'\n",
        "    df[\"Idoso_Com_Dependentes\"] = ((df[\"customer.SeniorCitizen\"] == 1) & (df[\"customer.Dependents\"] == \"Yes\")).astype(int)\n",
        "\n",
        "    # Drop irrelevant columns identified from EDA or common sense\n",
        "    cols_to_drop = [\n",
        "        \"customerID\", # Identifier\n",
        "    ]\n",
        "    df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df_processed = clean_and_engineer_features(df)\n",
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2600957",
      "metadata": {
        "id": "f2600957"
      },
      "source": [
        "## 4. Divisão de Dados e Pré-processamento com Pipeline\n",
        "\n",
        "Dividimos os dados em conjuntos de treino e teste e aplicamos transformações (escalonamento para numéricos e One-Hot Encoding para categóricos) usando um `ColumnTransformer` dentro de um `Pipeline` para garantir que o pré-processamento seja aplicado de forma consistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a0f370",
      "metadata": {
        "id": "13a0f370"
      },
      "outputs": [],
      "source": [
        "def split_data(df: pd.DataFrame, target_col: str = \"churn_flag\", test_size: float = 0.3, random_state: int = 42):\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = split_data(df_processed)\n",
        "\n",
        "# Identificar colunas categóricas e numéricas\n",
        "categorical_cols = X_train.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
        "numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Criar pipelines de pré-processamento para features numéricas e categóricas\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")\n",
        "\n",
        "# Criar um ColumnTransformer para aplicar diferentes transformações a diferentes colunas\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numerical_transformer, numerical_cols),\n",
        "        (\"cat\", categorical_transformer, categorical_cols),\n",
        "    ],\n",
        "    remainder=\"passthrough\" # Manter outras colunas (se houver) que não são transformadas\n",
        ")\n",
        "\n",
        "# Fit o preprocessor nos dados de treino\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Transformar dados de treino e teste\n",
        "X_train_processed = preprocessor.transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Salvar o preprocessor ajustado\n",
        "joblib.dump(preprocessor, MODELS_PATH / \"preprocessor.pkl\")\n",
        "\n",
        "print(\"Pré-processamento concluído. Dados transformados e pré-processador salvo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea9fff1b",
      "metadata": {
        "id": "ea9fff1b"
      },
      "source": [
        "## 5. Modelagem e Avaliação\n",
        "\n",
        "Nesta seção, treinamos e avaliamos diferentes modelos de Machine Learning (Regressão Logística, Random Forest, XGBoost) para a previsão de churn. As métricas de avaliação incluem Acurácia, Precisão, Recall, F1-Score e AUC, com foco especial em Recall e AUC para este problema de negócio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaeaed2f",
      "metadata": {
        "id": "eaeaed2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
        "    print(f\"\n",
        "--- Treinando e Avaliando {model_name} ---\")\n",
        "\n",
        "    # Aplicar SMOTE apenas nos dados de treino\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"\n",
        "Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\n",
        "Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Cross-validation\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=cv, scoring=\"roc_auc\")\n",
        "    print(f\"\n",
        "Cross-validation ROC AUC scores: {cv_scores}\")\n",
        "    print(f\"Mean CV ROC AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Salvar modelo\n",
        "    joblib.dump(model, MODELS_PATH / f\"model_{model_name.lower().replace(' ', '_')}.pkl\")\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"mean_cv_roc_auc\": np.mean(cv_scores)\n",
        "    }\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, solver=\"liblinear\"),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    result = train_and_evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test, name)\n",
        "    results.append(result)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_markdown(REPORTS_PATH / \"model_evaluation_summary.md\", index=False)\n",
        "print(f\"\n",
        "Sumário da avaliação dos modelos salvo em {REPORTS_PATH / 'model_evaluation_summary.md'}\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca99a092",
      "metadata": {
        "id": "ca99a092"
      },
      "source": [
        "## 6. Otimização do Modelo e Análise de Resultados\n",
        "\n",
        "Esta seção foca na otimização do modelo selecionado (Regressão Logística, devido ao seu bom recall e interpretabilidade) através do ajuste do threshold de classificação e na otimização de hiperparâmetros para o Random Forest como exemplo de melhoria de desempenho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05989dba",
      "metadata": {
        "id": "05989dba"
      },
      "outputs": [],
      "source": [
        "# Carregar o modelo de Regressão Logística treinado\n",
        "model_lr = joblib.load(MODELS_PATH / \"model_logistic_regression.pkl\")\n",
        "\n",
        "def find_optimal_threshold(model, X_test, y_test, target_recall=0.8):\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "    optimal_threshold = 0.5 # Default\n",
        "    for i in range(len(thresholds)):\n",
        "        if recalls[i] >= target_recall:\n",
        "            optimal_threshold = thresholds[i]\n",
        "            break\n",
        "\n",
        "    print(f\"Threshold para recall >= {target_recall}: {optimal_threshold:.4f}\")\n",
        "\n",
        "    # Avaliar com o threshold otimizado\n",
        "    y_pred_optimal = (y_proba >= optimal_threshold).astype(int)\n",
        "    print(\"\n",
        "Classification Report com Threshold Otimizado:\")\n",
        "    print(classification_report(y_test, y_pred_optimal))\n",
        "    print(\"\n",
        "Confusion Matrix com Threshold Otimizado:\")\n",
        "    print(confusion_matrix(y_test, y_pred_optimal))\n",
        "\n",
        "    return optimal_threshold\n",
        "\n",
        "print(\"\n",
        "### Ajuste de Threshold para Regressão Logística ###\")\n",
        "optimal_threshold_lr = find_optimal_threshold(model_lr, X_test_processed, y_test, target_recall=0.8)\n",
        "joblib.dump(optimal_threshold_lr, MODELS_PATH / \"optimal_threshold_lr.pkl\")\n",
        "\n",
        "def hyperparameter_tuning(model, X_train, y_train, param_distributions, n_iter=10, cv=5):\n",
        "    print(\"\n",
        "--- Otimização de Hiperparâmetros com RandomizedSearchCV ---\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=n_iter,\n",
        "        cv=cv,\n",
        "        scoring=\"roc_auc\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1, # Usar todos os cores disponíveis\n",
        "        verbose=1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "    print(f\"Melhores hiperparâmetros: {random_search.best_params_}\")\n",
        "    print(f\"Melhor ROC AUC (CV): {random_search.best_score_:.4f}\")\n",
        "    return random_search.best_estimator_\n",
        "\n",
        "# Aplicar SMOTE para o conjunto de treino antes do tuning\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled_tuning, y_train_resampled_tuning = smote.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "print(\"\n",
        "### Otimização de Hiperparâmetros para Random Forest ###\")\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "param_dist_rf = {\n",
        "    \"n_estimators\": randint(100, 500),\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],\n",
        "    \"max_depth\": randint(5, 30),\n",
        "    \"min_samples_split\": randint(2, 20),\n",
        "    \"min_samples_leaf\": randint(1, 20),\n",
        "    \"bootstrap\": [True, False]\n",
        "}\n",
        "best_rf_model = hyperparameter_tuning(rf_model, X_train_resampled_tuning, y_train_resampled_tuning, param_dist_rf, n_iter=20)\n",
        "joblib.dump(best_rf_model, MODELS_PATH / \"best_random_forest_model.pkl\")\n",
        "\n",
        "print(\"Otimização do modelo concluída.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d44cbb",
      "metadata": {
        "id": "f1d44cbb"
      },
      "source": [
        "## 7. Conclusão e Próximos Passos\n",
        "\n",
        "Este projeto demonstrou a construção de um pipeline completo para previsão de churn, desde a ingestão de dados até a otimização de modelos. O modelo de Regressão Logística, com seu threshold ajustado para priorizar o recall, mostrou-se promissor para identificar clientes em risco.\n",
        "\n",
        "**Próximos Passos:**\n",
        "*   **Implementação em Produção:** Integrar o modelo e o pré-processador em um sistema de produção para inferência em tempo real ou em lotes.\n",
        "*   **Monitoramento Contínuo:** Acompanhar o desempenho do modelo em produção e re-treiná-lo periodicamente com novos dados.\n",
        "*   **Análise de Custo-Benefício:** Avaliar o impacto financeiro das ações de retenção baseadas nas previsões do modelo.\n",
        "*   **Exploração de Outros Modelos:** Testar modelos mais avançados ou ensembles para potencial melhoria de desempenho.\n",
        "*   **Engenharia de Features Avançada:** Investigar a criação de features mais complexas ou o uso de técnicas de seleção de features.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}